# Проект «Климова-75-63»: Работа с данными, API и EDA

---

## О проекте

Проект объединяет задачи по управлению, обработке и аналитике данных, включая:  
- Формирование и анализ исходного датасета по древесине  
- Очистка, трансформация и сохранение данных
- Получение цитат через внешний API и их обработка    
- Проведение визуального и статистического анализа данных (EDA)  
- Запись данных в базу данных PostgreSQL

---

## Основные задачи

### 1. Формирование исходного датасета
- Сбор и структурирование первичных данных по биомассе  
- Сохранение исходного датасета в формате CSV

### 2. Первичный анализ и очистка данных
- Проверка дубликатов и пропусков  
- Приведение столбцов к нужным типам данных  
- Сохранение очищенных и подготовленных данных

### 3. Работа с обновлённым API цитат
- Получение данных о цитатах и авторах через API  
- Обработка и парсинг ответов  
- Скрипт для работы с API: `API_example/API_reader.py`  
- Результаты сохраняются в CSV

### 4. Обработка и визуализация древесных данных (EDA)
- Загрузка и трансформация исходных данных  
- Исследовательский анализ: графики, статистика, корреляции  
- Использование Jupyter Notebook из папки `notebooks/EDA.ipynb`  
- Сохранение промежуточных результатов в CSV и Parquet


### 5. Работа с базой данных PostgreSQL
- Надёжное и безопасное хранение данных в PostgreSQL  
- Использование `write_to_db.py` (главная ветка) для загрузки данных в таблицу  
- Учётные данные хранятся локально, подгружаются динамически для безопасности

---


## Структура проекта

Проект «Климова-75-63»

│

├── data_loader.py                # Скрипт обработки и загрузки данных о древесине

│

├── write_to_db.py                # Скрипт для автоматической записи данных в базу PostgreSQL

│

├── notebooks/                    # Папка с Jupyter Notebook для анализа и визуализации (EDA)

│   └── EDA.ipynb                 # Notebook для анализа данных

│

├── API_example/                  # Скрипты для работы с API цитат

│   └── API_reader.py             # Скрипт для скачивания, парсинга и сохранения цитат

│

├── requirements.txt              # Зависимости, список установленных библиотек

│

└── README.md                     # Документ с инструкциями и описанием проекта

---

## Данные

**Исходный деревесный датасет:**  
[Biomass Data](https://drive.google.com/drive/folders/1TOftr_GOVv2wXgeg4S5GTd46YWDHC2Ls?usp=drive_link)

**Данные цитат через API:**  
API доступен по адресу:  
`https://quotes.toscrape.com/api/quotes`  
Работа с API и сохранение результатов осуществляется с помощью  
`API_example/API_reader.py`.

**База данных PostgreSQL:**  
Обработанные данные загружаются в PostgreSQL через  
`write_to_db.py`, используемый в главной ветке.  
Все параметры подключения находятся в локальном безопасном хранилище.

---

### Примеры визуализации и вывода данных

**Пример EDA по биомассе:**

<img width="795" height="620" alt="screenshot" src="https://github.com/user-attachments/assets/345ff719-20e7-4dff-99b9-a32712106360" />


**Пример данных по цитатам:**

<img width="906" height="559" alt="image" src="https://github.com/user-attachments/assets/387d16b5-dee8-4dd9-8c02-794b3c4e2bc6" />


**ПРимер загрузки данные в DataBase**

<img width="903" height="294" alt="image" src="https://github.com/user-attachments/assets/1518cf2e-3257-4772-a6e2-28bce50bd5c4" />


---


## Быстрый старт

**1. Клонируйте репозиторий:**
 
    git clone <https://github.com/aclimova/Klimova-75-63-project>
    
    cd <Климова-75-63>
    
**2. (Рекомендуется) создайте и активируйте виртуальное окружение:**
 
    Для Windows:

        python -m venv venv
      
        venv\Scripts\activate
        
    Для macOS/Linux:
    
        python3 -m venv venv
       
        source venv/bin/activate
        
**3. Установите зависимости:**
  
    pip install -r requirements.txt
    
**4. Скачайте и подготовьте данные:**
   
    Запустите data_loader.py для автоматической загрузки и подготовки исходных данных.
    
    Все промежуточные датасеты будут сохранены в папке data/.
 
    python data_loader.py

**5. Для работы с внешним API и парсингом цитат используйте скрипты:**
    
    python API_example/API_reader.py
   
    Результаты сохраняются в CSV-файл.

**6. Запустите Jupyter Notebook и проведите EDA:** 
    
    jupyter notebook
   
    Откройте в браузере папку notebooks/, EDA.ipynb и пошагово выполните анализ.

    
**7. Загрузите данные в базу PostgreSQL:**

    python write_to_db.py
   
---

**Примечание:**

- Все настройки и примеры вывода размещены в README.md.

- Для работы используйте Python 3.8 и выше, все библиотеки перечислены в requirements.txt.

- Если встретите ошибки — перепроверьте пути и установленные зависимости.

- Никогда не выкладывайте файл с крэденшалами `creds.db` в публичных репозиториях.
